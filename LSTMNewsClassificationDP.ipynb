{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTMNewsClassificationDP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/K0XDYM9Jzk0x0oWfQASK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/DP-NLP/blob/main/LSTMNewsClassificationDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10DXTdvxj5ZQ",
        "outputId": "68620f53-e44f-4dc7-e1a6-362cca6e9caf"
      },
      "source": [
        "!pip install opacus[dev] --quiet\n",
        "!pip install transformers --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 102kB 2.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 25.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 21.6MB 1.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 40.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 49.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9MB 34.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 747kB 24.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 727kB 30.2MB/s \n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.12.0 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 5.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 41.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 44.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_owdx2pklf5"
      },
      "source": [
        "import zipfile\n",
        "import urllib.request\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "from opacus import PrivacyEngine\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers.data.processors.utils import InputExample\n",
        "from transformers.data.processors.glue import glue_convert_examples_to_features"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvd-mlWikysj"
      },
      "source": [
        "DATA_DIR = \"/content/\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pUsYekVkzXg"
      },
      "source": [
        "def download_and_extract(data_dir):\n",
        "    print(\"Extracting Train zip...\")\n",
        "    filename = \"train.csv.zip\"\n",
        "    with zipfile.ZipFile(filename) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    os.remove(filename)\n",
        "    print(\"Completed!\")\n",
        "\n",
        "    print(\"Extracting Test zip...\")\n",
        "    filename = \"test.csv.zip\"\n",
        "    with zipfile.ZipFile(filename) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    os.remove(filename)\n",
        "    print(\"Completed!\")\n",
        "\n",
        "download_and_extract(DATA_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4Bu52Cgk1fk"
      },
      "source": [
        "train_path =  '/content/train.csv'\n",
        "dev_path = '/content/test.csv'\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(dev_path)\n",
        "df_train = df_train.drop('Title', axis = 1)\n",
        "df_test = df_test.drop('Title', axis = 1)\n",
        "df_train = df_train.replace({'Class Index': {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}})\n",
        "df_test = df_test.replace({'Class Index': {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}})"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwJm9ozelyFH"
      },
      "source": [
        "class CharByteEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.start_token = \"<s>\"\n",
        "        self.end_token = \"</s>\"\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.start_idx = 256\n",
        "        self.end_idx = 257\n",
        "        self.pad_idx = 258\n",
        "\n",
        "    def forward(self, s: str, pad_to=0) -> torch.LongTensor:\n",
        "\n",
        "        encoded = s.encode()\n",
        "        n_pad = pad_to - len(encoded) if pad_to > len(encoded) else 0\n",
        "        return torch.LongTensor(\n",
        "            [self.start_idx]\n",
        "            + [c for c in encoded]  # noqa\n",
        "            + [self.end_idx]\n",
        "            + [self.pad_idx for _ in range(n_pad)]\n",
        "        )\n",
        "\n",
        "    def decode(self, char_ids_tensor: torch.LongTensor) -> str:\n",
        "        char_ids = char_ids_tensor.cpu().detach().tolist()\n",
        "\n",
        "        out = []\n",
        "        buf = []\n",
        "        for c in char_ids:\n",
        "            if c < 256:\n",
        "                buf.append(c)\n",
        "            else:\n",
        "                if buf:\n",
        "                    out.append(bytes(buf).decode())\n",
        "                    buf = []\n",
        "                if c == self.start_idx:\n",
        "                    out.append(self.start_token)\n",
        "                elif c == self.end_idx:\n",
        "                    out.append(self.end_token)\n",
        "                elif c == self.pad_idx:\n",
        "                    out.append(self.pad_token)\n",
        "\n",
        "        if buf:  # in case some are left\n",
        "            out.append(bytes(buf).decode())\n",
        "        return \"\".join(out)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        The length of our encoder space. This is fixed to 256 (one byte) + 3 special chars\n",
        "        (start, end, pad).\n",
        "        Returns:\n",
        "            259\n",
        "        \"\"\"\n",
        "        return 259"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjRndYj5nU_n"
      },
      "source": [
        "class NewsClassification(Dataset):\n",
        "    def __init__(self , df): # df with the news description and label \n",
        "        self.labels = df['Class Index']\n",
        "        self.data = df['Description']\n",
        "        self.labels_dict = {label: i for i, label in enumerate(self.labels)}\n",
        "        self.encoder = CharByteEncoder()\n",
        "        self.processed = self.process_samples()\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "      return self.processed[i]\n",
        "    \n",
        "    def process_samples(self):\n",
        "      processed = []\n",
        "      for d, l in zip(self.data, self.labels):\n",
        "        processed.append((self.encoder(d.strip()), self.encoder(l.strip())))\n",
        "      return processed\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "      \n",
        "\n",
        "VOCAB_SIZE = 256 + 3  # 256 alternatives in one byte, plus 3 special characters."
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr1h53Mnn373"
      },
      "source": [
        "secure_rng = False\n",
        "generator = None \n",
        "train_split = 0.8\n",
        "test_every = 5\n",
        "batch_size = 800"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP8Dnme-o3Gg"
      },
      "source": [
        "train_ds = NewsClassification(df_train)\n",
        "train_len = int(train_split * len(ds))\n",
        "test_ds = NewsClassification(df_test)\n",
        "test_len = len(ds) - train_len"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivHpf4jho6e0",
        "outputId": "49d208e0-2e17-452d-88e0-de98c3469e46"
      },
      "source": [
        "print(f\"{train_len} samples for training, {test_len} for testing\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "96000 samples for training, 24000 for testing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDRY3r-1mOn6"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def padded_collate(batch, padding_idx=0):\n",
        "    x = pad_sequence([elem[0] for elem in batch], batch_first=True, padding_value=padding_idx)\n",
        "    y = pad_sequence([elem[1] for elem in batch], batch_first=True, padding_value=padding_idx)\n",
        "\n",
        "    return x, y"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFtXV3HxoDIG"
      },
      "source": [
        "sample_rate = batch_size / len(train_ds)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        "    generator=generator,\n",
        "    batch_sampler=UniformWithReplacementSampler(\n",
        "        num_samples=len(train_ds),\n",
        "        sample_rate=sample_rate,\n",
        "        generator=generator,\n",
        "    ),\n",
        "    collate_fn=padded_collate,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=2 * batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=1,\n",
        "    pin_memory=True,\n",
        "    collate_fn=padded_collate,\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmQGndUIoN7f"
      },
      "source": [
        "from statistics import mean\n",
        "\n",
        "def train(model, criterion, optimizer, train_loader, epoch, device=\"cuda:0\"):\n",
        "    accs = []\n",
        "    losses = []\n",
        "    for x, y in tqdm(train_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        y = torch.max(y, 1)[1]\n",
        "\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        preds = logits.argmax(-1)\n",
        "        n_correct = float(preds.eq(y).sum())\n",
        "        batch_accuracy = n_correct / len(y)\n",
        "\n",
        "        accs.append(batch_accuracy)\n",
        "        losses.append(float(loss))\n",
        "\n",
        "    printstr = (\n",
        "        f\"\\t Epoch {epoch}. Accuracy: {mean(accs):.6f} | Loss: {mean(losses):.6f}\"\n",
        "    )\n",
        "    try:\n",
        "        privacy_engine = optimizer.privacy_engine\n",
        "        epsilon, best_alpha = privacy_engine.get_privacy_spent()\n",
        "        printstr += f\" | (ε = {epsilon:.2f}, δ = {privacy_engine.target_delta}) for α = {best_alpha}\"\n",
        "    except AttributeError:\n",
        "        pass\n",
        "    print(printstr)\n",
        "    return\n",
        "\n",
        "\n",
        "def test(model, test_loader, privacy_engine, device=\"cuda:0\"):\n",
        "    accs = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(test_loader):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            y = torch.max(y, 1)[1]\n",
        "\n",
        "            preds = model(x).argmax(-1)\n",
        "            n_correct = float(preds.eq(y).sum())\n",
        "            batch_accuracy = n_correct / len(y)\n",
        "\n",
        "            accs.append(batch_accuracy)\n",
        "    printstr = \"\\n----------------------------\\n\" f\"Test Accuracy: {mean(accs):.6f}\"\n",
        "    if privacy_engine:\n",
        "        epsilon, best_alpha = privacy_engine.get_privacy_spent()\n",
        "        printstr += f\" (ε = {epsilon:.2f}, δ = {privacy_engine.target_delta}) for α = {best_alpha}\"\n",
        "    print(printstr + \"\\n----------------------------\\n\")\n",
        "    return"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyoTeBGXoQDz"
      },
      "source": [
        "# Training hyper-parameters\n",
        "epochs = 50\n",
        "learning_rate = 2.0\n",
        "\n",
        "# Privacy engine hyper-parameters\n",
        "max_per_sample_grad_norm = 1.5\n",
        "delta = 8e-5\n",
        "epsilon = 12.0"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJoUY5YloZVM"
      },
      "source": [
        "from opacus.layers import DPLSTM\n",
        "\n",
        "class CharNNClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_size,\n",
        "        hidden_size,\n",
        "        output_size,\n",
        "        num_lstm_layers=1,\n",
        "        bidirectional=False,\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.lstm = DPLSTM(\n",
        "            embedding_size,\n",
        "            hidden_size,\n",
        "            num_layers=num_lstm_layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.out_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)  # -> [B, T, D]\n",
        "        x, _ = self.lstm(x, hidden)  # -> [B, T, H]\n",
        "        x = x[:, -1, :]  # -> [B, H]\n",
        "        x = self.out_layer(x)  # -> [B, C]\n",
        "        return x"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB6ygvmdob5i"
      },
      "source": [
        "# Set the device to run on a GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define classifier parameters\n",
        "embedding_size = 64\n",
        "hidden_size = 128  # Number of neurons in hidden layer after LSTM\n",
        "n_lstm_layers = 1\n",
        "bidirectional_lstm = False\n",
        "\n",
        "model = CharNNClassifier(\n",
        "    embedding_size,\n",
        "    hidden_size,\n",
        "    len(ds.labels),\n",
        "    n_lstm_layers,\n",
        "    bidirectional_lstm,\n",
        ").to(device)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1O860IioeJm"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8usVx9Bof_b",
        "outputId": "1f775672-a94e-4f4b-9128-8b3e4d17887d"
      },
      "source": [
        "from opacus import PrivacyEngine\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    model,\n",
        "    sample_rate=sample_rate,\n",
        "    max_grad_norm=max_per_sample_grad_norm,\n",
        "    target_delta=delta,\n",
        "    target_epsilon=epsilon,\n",
        "    epochs=epochs,\n",
        "    secure_rng=secure_rng,\n",
        ")\n",
        "privacy_engine.attach(optimizer)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df6me13noiu8"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Train stats: \\n\")\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    train(model, criterion, optimizer, train_loader, epoch, device=device)\n",
        "    if test_every:\n",
        "        if epoch % test_every == 0:\n",
        "            test(model, test_loader, privacy_engine, device=device)\n",
        "\n",
        "test(model, test_loader, privacy_engine, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfQNSKGHD8GL"
      },
      "source": [
        "model_nodp = CharNNClassifier(\n",
        "    embedding_size,\n",
        "    hidden_size,\n",
        "    len(ds.labels),\n",
        "    n_lstm_layers,\n",
        "    bidirectional_lstm,\n",
        ").to(device)\n",
        "\n",
        "\n",
        "optimizer_nodp = torch.optim.SGD(model_nodp.parameters(), lr=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ2_mG8AEpPM"
      },
      "source": [
        "for epoch in tqdm(range(epochs)):\n",
        "    train(model_nodp, criterion, optimizer_nodp, train_loader, epoch, device=device)\n",
        "    if test_every:\n",
        "        if epoch % test_every == 0:\n",
        "            test(model_nodp, test_loader, None, device=device)\n",
        "\n",
        "test(model_nodp, test_loader, None, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}