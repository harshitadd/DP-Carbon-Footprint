{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "BERTNewsClassificationDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/DP-NLP/blob/main/BERTNewsClassificationDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw1v96eaKGMn"
      },
      "source": [
        "%%bash \n",
        "pip install torchcsprng==0.1.3+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "git clone https://github.com/pytorch/opacus.git  \n",
        "cd opacus\n",
        "pip install -e ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUjkHKMQzya2"
      },
      "source": [
        "! pip install transformers --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ZQ3R2akTxa"
      },
      "source": [
        "! pip install codecarbon --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Bjc65TOw3P"
      },
      "source": [
        "exit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcwKLKohz4Oy"
      },
      "source": [
        "import zipfile\n",
        "import urllib.request\n",
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import transformers\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "from opacus import PrivacyEngine\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers.data.processors.utils import InputExample\n",
        "from transformers.data.processors.glue import glue_convert_examples_to_features\n",
        "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
        "from codecarbon import OfflineEmissionsTracker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1peQssC_jU0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8071dc19-68fc-44fe-f383-0c528c9f7655"
      },
      "source": [
        "tracker = OfflineEmissionsTracker(country_iso_code=\"CAN\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CODECARBON : No CPU tracking mode found. Falling back on CPU constant mode.\n",
            "CODECARBON : Failed to match CPU TDP constant. Falling back on a global constant.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPPd7_8AxYCx"
      },
      "source": [
        "DATA_DIR = \"/content/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSBaJ4yOxYCy"
      },
      "source": [
        "def download_and_extract(data_dir):\n",
        "  print(\"Extracting Train zip...\")\n",
        "  filename = \"train.csv.zip\"\n",
        "  with zipfile.ZipFile(filename) as zip_ref:\n",
        "      zip_ref.extractall(data_dir)\n",
        "  os.remove(filename)\n",
        "  print(\"Completed!\")\n",
        "\n",
        "  print(\"Extracting Test zip...\")\n",
        "  filename = \"test.csv.zip\"\n",
        "  with zipfile.ZipFile(filename) as zip_ref:\n",
        "      zip_ref.extractall(data_dir)\n",
        "  os.remove(filename)\n",
        "  print(\"Completed!\")\n",
        "\n",
        "download_and_extract(DATA_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "746u8P-KxYC2"
      },
      "source": [
        "train_path =  '/content/train.csv'\n",
        "dev_path = '/content/test.csv'\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(dev_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgPvnvuoA0o0"
      },
      "source": [
        "df_train = df_train.drop('Title', axis = 1)[:15000]\n",
        "df_test = df_test.drop('Title', axis = 1)[:3000]\n",
        "df_train = df_train.replace({'Class Index': {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}})\n",
        "df_test = df_test.replace({'Class Index': {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsBvdDtLxYC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a980fe-3b57-40e0-be72-5de0a50dc107"
      },
      "source": [
        "model_name = \"bert-base-cased\"\n",
        "config = BertConfig.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=4,\n",
        ")\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    do_lower_case=False,\n",
        ")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    config=config,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfSVd-LRxYC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3b24f5-8317-4d84-d890-9fdf99b7b000"
      },
      "source": [
        "# trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n",
        "trainable_layers = [model.classifier]\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "\n",
        "for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "        total_params += p.numel()\n",
        "\n",
        "for layer in trainable_layers:\n",
        "    for p in layer.parameters():\n",
        "        p.requires_grad = True\n",
        "        trainable_params += p.numel()\n",
        "\n",
        "print(f\"Total parameters count: {total_params}\") # ~108M\n",
        "print(f\"Trainable parameters count: {trainable_params}\") # ~7M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total parameters count: 108313348\n",
            "Trainable parameters count: 3076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJoinsVsxYC7"
      },
      "source": [
        "LABEL_LIST = ['World','Sports','Business','Sci/Tech'] \n",
        "MAX_SEQ_LENGHT = 128\n",
        "\n",
        "\n",
        "def _create_examples(df, set_type):\n",
        "    \"\"\" Convert raw dataframe to a list of InputExample. Filter malformed examples\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    for index, row in df.iterrows():\n",
        "        guid = f\"{index}-{set_type}\"\n",
        "        examples.append(\n",
        "            InputExample(guid=guid, text_a=row['Description'], label=row['Class Index']))\n",
        "    return examples\n",
        "\n",
        "def _df_to_features(df, set_type):\n",
        "    examples = _create_examples(df, set_type)\n",
        "    legacy_kwards = {}\n",
        "    from packaging import version\n",
        "    if version.parse(transformers.__version__) < version.parse(\"2.9.0\"):\n",
        "        legacy_kwards = {\n",
        "            \"pad_on_left\": False,\n",
        "            \"pad_token\": tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "            \"pad_token_segment_id\": 0,\n",
        "        }\n",
        "    \n",
        "    return glue_convert_examples_to_features(\n",
        "        examples=examples,\n",
        "        tokenizer=tokenizer,\n",
        "        label_list=LABEL_LIST,\n",
        "        max_length=MAX_SEQ_LENGHT,\n",
        "        output_mode=\"classification\",\n",
        "        **legacy_kwards,\n",
        "    )\n",
        "\n",
        "def _features_to_dataset(features):\n",
        "    \"\"\" Convert features from `_df_to_features` into a single dataset\n",
        "    \"\"\"\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor(\n",
        "        [f.attention_mask for f in features], dtype=torch.long\n",
        "    )\n",
        "    all_token_type_ids = torch.tensor(\n",
        "        [f.token_type_ids for f in features], dtype=torch.long\n",
        "    )\n",
        "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    dataset = TensorDataset(\n",
        "        all_input_ids, all_attention_mask, all_token_type_ids, all_labels\n",
        "    )\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eQH_x535pyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0feb0c3e-9b2e-4299-d963-3411d443ac86"
      },
      "source": [
        "train_features = _df_to_features(df_train, \"train\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wWZVxey_KD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eaf64a8-8626-4bda-f1cd-5326eedac9c8"
      },
      "source": [
        "test_features = _df_to_features(df_test, \"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoq98IjD5rj_"
      },
      "source": [
        "train_dataset = _features_to_dataset(train_features)\n",
        "test_dataset = _features_to_dataset(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LfyCd1cxYC_"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "VIRTUAL_BATCH_SIZE = 32\n",
        "assert VIRTUAL_BATCH_SIZE % BATCH_SIZE == 0\n",
        "N_ACCUMULATION_STEPS = int(VIRTUAL_BATCH_SIZE / BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_YFUd-0xYDA"
      },
      "source": [
        "\n",
        "SAMPLE_RATE = BATCH_SIZE / len(train_dataset)\n",
        "\n",
        "train_sampler=UniformWithReplacementSampler(\n",
        "    num_samples=len(train_dataset),\n",
        "    sample_rate=SAMPLE_RATE,\n",
        ")\n",
        "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
        "\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLF4RNzLxYDA"
      },
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "model = model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnRl3xmwxYDB"
      },
      "source": [
        "EPOCHS = 10\n",
        "LOGGING_INTERVAL = 1000 \n",
        "EPSILON = 5\n",
        "DELTA = 1 / len(train_dataloader) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZidBtYS9xYDC"
      },
      "source": [
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "def count_parameters(model):\n",
        "    print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "def evaluate(model, test_acc):    \n",
        "    model.eval()\n",
        "\n",
        "    loss_arr = []\n",
        "    accuracy_arr = []\n",
        "    \n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2],\n",
        "                      'labels':         batch[3]}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss, logits = outputs[:2]\n",
        "            \n",
        "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            labels = inputs['labels'].detach().cpu().numpy()\n",
        "            \n",
        "            loss_arr.append(loss.item())\n",
        "            test_acc.append(accuracy(preds, labels))\n",
        "            accuracy_arr.append(accuracy(preds, labels))\n",
        "    \n",
        "    model.train()\n",
        "    return np.mean(loss_arr), np.mean(accuracy_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utJAmqlXxYDD"
      },
      "source": [
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    module=model,\n",
        "    sample_rate=SAMPLE_RATE * N_ACCUMULATION_STEPS,\n",
        "    target_delta = DELTA,\n",
        "    target_epsilon = EPSILON, \n",
        "    epochs = EPOCHS,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "privacy_engine.attach(optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v76vtUtxYDD"
      },
      "source": [
        "# Privacy Variant \n",
        "tracker.start()\n",
        "test_acc = []\n",
        "train_acc = []\n",
        "threshold_test_accuracy = 0.52\n",
        "count_parameters(model)\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    losses = []\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        \n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'token_type_ids': batch[2],\n",
        "                  'labels':         batch[3]}\n",
        "        outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n",
        "        loss, logits = outputs[:2]  \n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % N_ACCUMULATION_STEPS == 0 or step == len(train_dataloader) - 1:\n",
        "            losses.append(loss.item())\n",
        "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            labels = inputs['labels'].detach().cpu().numpy()\n",
        "            train_acc.append(accuracy(preds, labels))\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            optimizer.virtual_step()\n",
        "\n",
        "        train_loss = np.mean(losses)\n",
        "\n",
        "        if step > 0 and step % LOGGING_INTERVAL == 0:\n",
        "     \n",
        "            eps, alpha = optimizer.privacy_engine.get_privacy_spent(DELTA)\n",
        "\n",
        "            eval_loss, eval_accuracy = evaluate(model, test_acc)\n",
        "            print(\n",
        "                f\"Epoch: {epoch} | \"\n",
        "                f\"Step: {step} | \"\n",
        "                f\"Train loss: {train_loss:.3f} | \"\n",
        "                f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "\n",
        "                )\n",
        "        \n",
        "    if eval_accuracy >= threshold_test_accuracy:\n",
        "      print('Threshold Accuracy Reached') \n",
        "      break \n",
        "\n",
        "    \n",
        "\n",
        "tracker.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQrtuRHNb8Da"
      },
      "source": [
        "train = [train_acc[x]*100 for x in range(len(train_acc))]\n",
        "test = [test_acc[x]*100 for x in range(len(test_acc))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNG5kV45Nkjh"
      },
      "source": [
        "train_fname = '/content/train_acc_vanilla.bin'\n",
        "with open(train_fname,'wb') as file:\n",
        "  pickle.dump(train_acc,file)\n",
        "test_fname = '/content/test_acc_vanilla.bin'\n",
        "with open(test_fname,'wb') as file:\n",
        "  pickle.dump(test_acc,file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}