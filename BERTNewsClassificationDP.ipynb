{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "BERTNewsClassificationDP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7f57ef386dc4ab3b514f4a40ea967d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_759c39ae1956406b9873f43ba16adc32",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_730d99170f504c5a954ca531d52eb53f",
              "IPY_MODEL_2ffb46c94c7d4495ab6a9ccaec1deb7b"
            ]
          }
        },
        "759c39ae1956406b9873f43ba16adc32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "730d99170f504c5a954ca531d52eb53f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_04316a7e2b9e46ea9edabd60a4872ac4",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 15000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_19a5c0eef5e5467fb935a9dba4482651"
          }
        },
        "2ffb46c94c7d4495ab6a9ccaec1deb7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_698f727a1c39464eb737b04e5f7eb2aa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/15000 [00:04&lt;3:47:34,  1.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3ef0f5c08c5048df89bf9f4325374f63"
          }
        },
        "04316a7e2b9e46ea9edabd60a4872ac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "19a5c0eef5e5467fb935a9dba4482651": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "698f727a1c39464eb737b04e5f7eb2aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3ef0f5c08c5048df89bf9f4325374f63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitadd/DP-NLP/blob/main/BERTNewsClassificationDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiFkWiSSzfFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0bdd16-f5eb-4a9f-fbce-ccfbbda00ba5"
      },
      "source": [
        "!pip install opacus[dev] --quiet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–                            | 10kB 721kB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 20kB 1.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 71kB 2.6MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 81kB 3.0MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 92kB 3.3MB/s eta 0:00:01\r\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102kB 2.8MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 7.0MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81kB 8.6MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.6MB 1.5MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 50.7MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143kB 57.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.9MB 18.5MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 368kB 39.1MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 7.7MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 6.6MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 747kB 28.6MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 727kB 36.6MB/s \n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sphinx-autodoc-typehints 1.12.0 has requirement Sphinx>=3.0, but you'll have sphinx 1.8.5 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUjkHKMQzya2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e284e6b-b607-42ed-8df6-768b7260e1c7"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.3MB 5.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 32.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 26.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcwKLKohz4Oy"
      },
      "source": [
        "import zipfile\n",
        "import urllib.request\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
        "from opacus import PrivacyEngine\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers.data.processors.utils import InputExample\n",
        "from transformers.data.processors.glue import glue_convert_examples_to_features\n",
        "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPPd7_8AxYCx"
      },
      "source": [
        "DATA_DIR = \"/content/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSBaJ4yOxYCy"
      },
      "source": [
        "def download_and_extract(data_dir):\n",
        "    print(\"Extracting Train zip...\")\n",
        "    filename = \"train.csv.zip\"\n",
        "    with zipfile.ZipFile(filename) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    os.remove(filename)\n",
        "    print(\"Completed!\")\n",
        "\n",
        "    print(\"Extracting Test zip...\")\n",
        "    filename = \"test.csv.zip\"\n",
        "    with zipfile.ZipFile(filename) as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "    os.remove(filename)\n",
        "    print(\"Completed!\")\n",
        "\n",
        "download_and_extract(DATA_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "746u8P-KxYC2"
      },
      "source": [
        "train_path =  '/content/train.csv'\n",
        "dev_path = '/content/test.csv'\n",
        "\n",
        "df_train = pd.read_csv(train_path)\n",
        "df_test = pd.read_csv(dev_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wusCfuudApaE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "02537725-ffa4-46c4-c720-28b8b9f77a63"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class Index</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
              "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
              "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Class Index  ...                                        Description\n",
              "0            3  ...  Reuters - Short-sellers, Wall Street's dwindli...\n",
              "1            3  ...  Reuters - Private investment firm Carlyle Grou...\n",
              "2            3  ...  Reuters - Soaring crude prices plus worries\\ab...\n",
              "3            3  ...  Reuters - Authorities have halted oil export\\f...\n",
              "4            3  ...  AFP - Tearaway world oil prices, toppling reco...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgPvnvuoA0o0"
      },
      "source": [
        "df_train = df_train.drop('Title', axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxwqY07OAqVw"
      },
      "source": [
        "df_test = df_test.drop('Title', axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXdpzBKQBLNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96cdd2b-7f08-442d-88f9-9b99886811e4"
      },
      "source": [
        "df_train.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Class Index', 'Description'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji2g2-7CBTv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686bf306-9e45-478d-d903-4fbd639b070f"
      },
      "source": [
        "df_test.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Class Index', 'Description'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk9MSY4KAbDG"
      },
      "source": [
        "# 1-World, 2-Sports, 3-Business, 4-Sci/Tech"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izm-dh0MD3ns"
      },
      "source": [
        "df_train = df_train.replace({'Class Index': {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM9CokTgE5EU"
      },
      "source": [
        "df_test = df_test.replace({'Class Index': {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcn3G-aGHu4m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "7951db07-bde8-4850-c6d0-09266d881fc9"
      },
      "source": [
        "df_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class Index</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>Unions representing workers at Turner   Newall...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sci/Tech</td>\n",
              "      <td>SPACE.com - TORONTO, Canada -- A second\\team o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sci/Tech</td>\n",
              "      <td>AP - A company founded by a chemistry research...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sci/Tech</td>\n",
              "      <td>AP - It's barely dawn when Mike Fitzpatrick st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sci/Tech</td>\n",
              "      <td>AP - Southern California's smog-fighting agenc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7595</th>\n",
              "      <td>World</td>\n",
              "      <td>Ukrainian presidential candidate Viktor Yushch...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7596</th>\n",
              "      <td>Sports</td>\n",
              "      <td>With the supply of attractive pitching options...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7597</th>\n",
              "      <td>Sports</td>\n",
              "      <td>Like Roger Clemens did almost exactly eight ye...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7598</th>\n",
              "      <td>Business</td>\n",
              "      <td>SINGAPORE : Doctors in the United States have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7599</th>\n",
              "      <td>Business</td>\n",
              "      <td>EBay plans to buy the apartment and home renta...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7600 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Class Index                                        Description\n",
              "0       Business  Unions representing workers at Turner   Newall...\n",
              "1       Sci/Tech  SPACE.com - TORONTO, Canada -- A second\\team o...\n",
              "2       Sci/Tech  AP - A company founded by a chemistry research...\n",
              "3       Sci/Tech  AP - It's barely dawn when Mike Fitzpatrick st...\n",
              "4       Sci/Tech  AP - Southern California's smog-fighting agenc...\n",
              "...          ...                                                ...\n",
              "7595       World  Ukrainian presidential candidate Viktor Yushch...\n",
              "7596      Sports  With the supply of attractive pitching options...\n",
              "7597      Sports  Like Roger Clemens did almost exactly eight ye...\n",
              "7598    Business  SINGAPORE : Doctors in the United States have ...\n",
              "7599    Business  EBay plans to buy the apartment and home renta...\n",
              "\n",
              "[7600 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsBvdDtLxYC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74955ef-c10b-49e7-a301-9d8067cd2ac2"
      },
      "source": [
        "model_name = \"bert-base-cased\"\n",
        "config = BertConfig.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=4,\n",
        ")\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    do_lower_case=False,\n",
        ")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    config=config,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfSVd-LRxYC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4204607b-7f12-4a72-8487-e0ab54abcbe2"
      },
      "source": [
        "trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]\n",
        "total_params = 0\n",
        "trainable_params = 0\n",
        "\n",
        "for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "        total_params += p.numel()\n",
        "\n",
        "for layer in trainable_layers:\n",
        "    for p in layer.parameters():\n",
        "        p.requires_grad = True\n",
        "        trainable_params += p.numel()\n",
        "\n",
        "print(f\"Total parameters count: {total_params}\") # ~108M\n",
        "print(f\"Trainable parameters count: {trainable_params}\") # ~7M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total parameters count: 108313348\n",
            "Trainable parameters count: 7681540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJoinsVsxYC7"
      },
      "source": [
        "################################################################################\n",
        "LABEL_LIST = ['World','Sports','Business','Sci/Tech'] \n",
        "MAX_SEQ_LENGHT = 128\n",
        "\n",
        "\n",
        "def _create_examples(df, set_type):\n",
        "    \"\"\" Convert raw dataframe to a list of InputExample. Filter malformed examples\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    for index, row in df.iterrows():\n",
        "        # if row['Category'] in LABEL_LIST:\n",
        "        #     continue\n",
        "        # if isinstance(row['ArticleId'], str) or isinstance(row['Text'], str):\n",
        "        #     continue\n",
        "            \n",
        "        guid = f\"{index}-{set_type}\"\n",
        "        examples.append(\n",
        "            InputExample(guid=guid, text_a=row['Description'], label=row['Class Index']))\n",
        "    return examples\n",
        "\n",
        "def _df_to_features(df, set_type):\n",
        "    \"\"\" Pre-process text. This method will:\n",
        "    1) tokenize inputs\n",
        "    2) cut or pad each sequence to MAX_SEQ_LENGHT\n",
        "    3) convert tokens into ids\n",
        "    \n",
        "    The output will contain:\n",
        "    `input_ids` - padded token ids sequence\n",
        "    `attention mask` - mask indicating padded tokens\n",
        "    `token_type_ids` - mask indicating the split between premise and hypothesis\n",
        "    `label` - label\n",
        "    \"\"\"\n",
        "    examples = _create_examples(df, set_type)\n",
        "    \n",
        "    #backward compatibility with older transformers versions\n",
        "    legacy_kwards = {}\n",
        "    from packaging import version\n",
        "    if version.parse(transformers.__version__) < version.parse(\"2.9.0\"):\n",
        "        legacy_kwards = {\n",
        "            \"pad_on_left\": False,\n",
        "            \"pad_token\": tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "            \"pad_token_segment_id\": 0,\n",
        "        }\n",
        "    \n",
        "    return glue_convert_examples_to_features(\n",
        "        examples=examples,\n",
        "        tokenizer=tokenizer,\n",
        "        label_list=LABEL_LIST,\n",
        "        max_length=MAX_SEQ_LENGHT,\n",
        "        output_mode=\"classification\",\n",
        "        **legacy_kwards,\n",
        "    )\n",
        "\n",
        "def _features_to_dataset(features):\n",
        "    \"\"\" Convert features from `_df_to_features` into a single dataset\n",
        "    \"\"\"\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor(\n",
        "        [f.attention_mask for f in features], dtype=torch.long\n",
        "    )\n",
        "    all_token_type_ids = torch.tensor(\n",
        "        [f.token_type_ids for f in features], dtype=torch.long\n",
        "    )\n",
        "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    dataset = TensorDataset(\n",
        "        all_input_ids, all_attention_mask, all_token_type_ids, all_labels\n",
        "    )\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eQH_x535pyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735b7ba0-6d6f-4915-e590-ee7759096ef4"
      },
      "source": [
        "train_features = _df_to_features(df_train, \"train\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wWZVxey_KD6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e535838-a5f0-4690-cb4d-384bc3c607cc"
      },
      "source": [
        "test_features = _df_to_features(df_test, \"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/processors/glue.py:67: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoq98IjD5rj_"
      },
      "source": [
        "train_dataset = _features_to_dataset(train_features)\n",
        "test_dataset = _features_to_dataset(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LfyCd1cxYC_"
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "VIRTUAL_BATCH_SIZE = 8\n",
        "assert VIRTUAL_BATCH_SIZE % BATCH_SIZE == 0 # VIRTUAL_BATCH_SIZE should be divisible by BATCH_SIZE\n",
        "N_ACCUMULATION_STEPS = int(VIRTUAL_BATCH_SIZE / BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_YFUd-0xYDA"
      },
      "source": [
        "\n",
        "SAMPLE_RATE = BATCH_SIZE / len(train_dataset)\n",
        "\n",
        "train_sampler=UniformWithReplacementSampler(\n",
        "    num_samples=len(train_dataset),\n",
        "    sample_rate=SAMPLE_RATE,\n",
        ")\n",
        "train_dataloader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
        "\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLF4RNzLxYDA"
      },
      "source": [
        "# Move the model to appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Set the model to train mode (HuggingFace models load in eval mode)\n",
        "model = model.train()\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cduq0usSJM5W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dae047a-6c6b-48eb-f84a-e6cf56869972"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnRl3xmwxYDB"
      },
      "source": [
        "EPOCHS = 1\n",
        "LOGGING_INTERVAL = 1000 # once every how many steps we run evaluation cycle and report metrics\n",
        "EPSILON = 7.5\n",
        "DELTA = 1 / len(train_dataloader) # Parameter for privacy accounting. Probability of not achieving privacy guarantees"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZidBtYS9xYDC"
      },
      "source": [
        "\n",
        "def accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "# define evaluation cycle\n",
        "def evaluate(model):    \n",
        "    model.eval()\n",
        "\n",
        "    loss_arr = []\n",
        "    accuracy_arr = []\n",
        "    \n",
        "    for batch in test_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2],\n",
        "                      'labels':         batch[3]}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss, logits = outputs[:2]\n",
        "            \n",
        "            preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n",
        "            labels = inputs['labels'].detach().cpu().numpy()\n",
        "            \n",
        "            loss_arr.append(loss.item())\n",
        "            accuracy_arr.append(accuracy(preds, labels))\n",
        "    \n",
        "    model.train()\n",
        "    return np.mean(loss_arr), np.mean(accuracy_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utJAmqlXxYDD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48344077-a799-46d0-8537-a154b3bba490"
      },
      "source": [
        "MAX_GRAD_NORM = 0.1\n",
        "\n",
        "privacy_engine = PrivacyEngine(\n",
        "    module=model,\n",
        "    sample_rate=SAMPLE_RATE * N_ACCUMULATION_STEPS,\n",
        "    target_delta = DELTA,\n",
        "    target_epsilon = EPSILON, \n",
        "    epochs = EPOCHS,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        ")\n",
        "privacy_engine.attach(optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:523: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
            "  \"A ``sample_rate`` has been provided.\"\n",
            "/usr/local/lib/python3.7/dist-packages/opacus/privacy_engine.py:195: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
            "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v76vtUtxYDD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "a7f57ef386dc4ab3b514f4a40ea967d5",
            "759c39ae1956406b9873f43ba16adc32",
            "730d99170f504c5a954ca531d52eb53f",
            "2ffb46c94c7d4495ab6a9ccaec1deb7b",
            "04316a7e2b9e46ea9edabd60a4872ac4",
            "19a5c0eef5e5467fb935a9dba4482651",
            "698f727a1c39464eb737b04e5f7eb2aa",
            "3ef0f5c08c5048df89bf9f4325374f63"
          ]
        },
        "outputId": "14e0bddf-af2f-4894-8b9f-99131d7510d6"
      },
      "source": [
        "for epoch in range(1, EPOCHS+1):\n",
        "    losses = []\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        \n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'token_type_ids': batch[2],\n",
        "                  'labels':         batch[3]}\n",
        "        outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # We process small batches of size BATCH_SIZE, \n",
        "        # until they're accumulated to a batch of size VIRTUAL_BATCH_SIZE.\n",
        "        # Only then we make a real `.step()` and update model weights\n",
        "        if (step + 1) % N_ACCUMULATION_STEPS == 0 or step == len(train_dataloader) - 1:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            optimizer.virtual_step()\n",
        "\n",
        "        if step > 0 and step % LOGGING_INTERVAL == 0:\n",
        "            train_loss = np.mean(losses)\n",
        "            eps, alpha = optimizer.privacy_engine.get_privacy_spent(DELTA)\n",
        "\n",
        "            eval_loss, eval_accuracy = evaluate(model)\n",
        "\n",
        "            print(\n",
        "                f\"Epoch: {epoch} | \"\n",
        "                f\"Step: {step} | \"\n",
        "                f\"Train loss: {train_loss:.3f} | \"\n",
        "                f\"Eval loss: {eval_loss:.3f} | \"\n",
        "                f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
        "                f\"É›: {eps:.2f} (Î±: {alpha})\"\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7f57ef386dc4ab3b514f4a40ea967d5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=15000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 0, 0, 0, 0, 1, 3, 3], device='cuda:0')\n",
            "tensor([2, 2, 1, 2, 3, 3, 2, 0], device='cuda:0')\n",
            "tensor([0, 2, 3, 2, 2], device='cuda:0')\n",
            "tensor([0, 1, 2, 2, 1, 0, 0, 1], device='cuda:0')\n",
            "tensor([0, 1, 2, 3, 2, 1, 2, 1, 0, 3, 2, 0], device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-fd37ca81757f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opacus/autograd_grad_sample.py\u001b[0m in \u001b[0;36m_capture_backprops\u001b[0;34m(layer, inputs, outputs, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mbackprops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0m_compute_grad_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_reduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opacus/autograd_grad_sample.py\u001b[0m in \u001b[0;36m_compute_grad_sample\u001b[0;34m(layer, backprops, loss_reduction, batch_first)\u001b[0m\n\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mcompute_layer_grad_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/opacus/supported_layers_grad_samplers.py\u001b[0m in \u001b[0;36m_compute_linear_grad_sample\u001b[0;34m(layer, A, B, batch_dim)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mbatch_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBatch\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n...i,n...j->n...ij\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     _create_or_extend_grad_sample(\n\u001b[1;32m     87\u001b[0m         \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n...ij->nij\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 13.50 GiB (GPU 0; 11.17 GiB total capacity; 642.65 MiB already allocated; 10.06 GiB free; 698.00 MiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymbZQyWoFCa7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}